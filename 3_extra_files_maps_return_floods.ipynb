{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bf9c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "PATH_DATA = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "PATH_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c136f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shapefile as shp\n",
    "import osmnx as ox\n",
    "import contextily as ctx\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D  # for legend handle\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from shapely.geometry import Point\n",
    "from pyproj import Proj, transform\n",
    "import math\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), \"../src\"))\n",
    "import functions_support as fsupport\n",
    "\n",
    "import importlib\n",
    "\n",
    "importlib.reload(fsupport)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf25245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_fechas(num_dias=2880):\n",
    "    fecha_base = datetime(2010, 1, 1)\n",
    "    fechas = [fecha_base + timedelta(days=i) for i in range(num_dias)]\n",
    "    return fechas\n",
    "\n",
    "# Define a hashing function\n",
    "def hash_geometry(geometry):\n",
    "    # Convert the geometry to a string representation\n",
    "    geometry_str = str(geometry)\n",
    "    # Hash the string representation using SHA-256\n",
    "    hashed_geometry = hashlib.sha256(geometry_str.encode()).hexdigest()\n",
    "    return hashed_geometry\n",
    "\n",
    "def group_hospitals_by_hosp_month_year(dataframe):\n",
    "        fechas_generadas = generar_fechas(dataframe['day'].max())\n",
    "        dataframe['date'] = dataframe['day'].apply(lambda x: fechas_generadas[x-1])\n",
    "        dataframe['year'] = dataframe['date'].dt.year\n",
    "        # dataframe['month'] = dataframe['date'].dt.month\n",
    "        \n",
    "        grouped_data = dataframe.groupby(['Hospital', \n",
    "                                        'year', \n",
    "                                        #   'month',\n",
    "                                        ]\n",
    "                                        ).agg(\n",
    "                                            temperature=(\"temperature\", \"mean\"),\n",
    "                                            # temperature_mean=(\"temperature\", \"mean\"),\n",
    "                                            # temperature_std=(\"temperature\", \"std\"),\n",
    "                                            # temperature_max=(\"temperature\", \"max\"),\n",
    "                                            # temperature_min=(\"temperature\", \"min\"),\n",
    "                                        ).reset_index()\n",
    "        return grouped_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0521fa6",
   "metadata": {},
   "source": [
    "## Global variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf8b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY_NAME = \"peru\"\n",
    "RESULT_DIR = \"results_RETURN_FLOODS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = PATH_DATA + \"/data/\" + COUNTRY_NAME + \"/\" + RESULT_DIR\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "directory_temperature = f\"{directory}/Temperature\"\n",
    "if not os.path.exists(directory_temperature):\n",
    "    os.makedirs(directory_temperature)\n",
    "    \n",
    "directory_prioritization = f\"{directory}/Prioritization\"\n",
    "if not os.path.exists(directory_prioritization):\n",
    "    os.makedirs(directory_prioritization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddb07092",
   "metadata": {},
   "source": [
    "## Administrative areas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eecace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (\n",
    "    PATH_DATA + \"/data/\" + COUNTRY_NAME + \"/preprocessed_sources/administrative_region\"\n",
    ")\n",
    "files = os.listdir(path)\n",
    "\n",
    "administrative_areas_dict = dict()\n",
    "\n",
    "for i in files:\n",
    "    name = fsupport.capitalize_string(i[:-8])\n",
    "    # adminitration = fsupport.capitalize_string(i[-9:-8])\n",
    "    # # key_tag = f'{name} {fsupport.category_dict[adminitration]}'\n",
    "    # key_tag = f'{name}{adminitration}'\n",
    "    administrative_areas_dict[name] = gpd.read_file(path + \"/\" + i)\n",
    "\n",
    "administrative_areas_dict.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bee836f4",
   "metadata": {},
   "source": [
    "## Temperatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kelvin_to_celsius(kelvin):\n",
    "    celsius = kelvin - 273.15\n",
    "    return celsius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50024a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{PATH_DATA}/data/{COUNTRY_NAME}/original_sources/temperatura/peru_salida_shape.shp\"\n",
    "path = f\"{PATH_DATA}/data/{COUNTRY_NAME}/temperatura_mensual_peru_2010_2019/temperatura_mensual_peru_2010_2019.shp\"\n",
    "\n",
    "temp_shape = gpd.read_file(path)\n",
    "\n",
    "temp_shape = temp_shape.rename(\n",
    "    columns={\"value\": \"temperature\",\n",
    "             \"band\": \"day\"}\n",
    ")                              \n",
    "\n",
    "temp_shape['temperature'] = temp_shape['temperature'].apply(lambda x: kelvin_to_celsius(x))\n",
    "\n",
    "print(len(temp_shape))\n",
    "temp_shape.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfeba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "columna_color = 'temperature'\n",
    "time_unit = sorted(set(temp_shape['day'])) #[:24*1] #[:24*33]\n",
    "\n",
    "temperaturas_min = temp_shape[columna_color].min()\n",
    "temperaturas_max = temp_shape[columna_color].max()\n",
    "\n",
    "def animate(i):\n",
    "    sub_data = temp_shape[temp_shape['day'] == time_unit[i]]\n",
    "    \n",
    "    ax.clear()\n",
    "    # ax.set_xlim([-82.0, -66.0]) # Límites en la coordenada X\n",
    "    # ax.set_ylim([-4.9, 14.0]) # Límites en la coordenada Y\n",
    "    sub_data.plot(column=columna_color, ax=ax, legend=False, cmap='RdYlGn_r', vmin=temperaturas_min, vmax=temperaturas_max)\n",
    "    ax.set_title('Day {}'.format(time_unit[i]))\n",
    "    return ax\n",
    "\n",
    "ani = FuncAnimation(fig, animate, frames=len(time_unit), repeat=True)\n",
    "\n",
    "ani.save(f'{directory_temperature}/video_day_temperature.gif', writer='ffmpeg', fps=24*3, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_shape['grid'] = temp_shape['geometry'] \n",
    "temp_shape['geometry'] = temp_shape['geometry'].centroid\n",
    "temp_shape['hashed_geometry'] = temp_shape['grid'].apply(hash_geometry)\n",
    "temp_shape.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9868a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = (\n",
    "        temp_shape.groupby(\n",
    "            [\n",
    "                \"hashed_geometry\",\n",
    "            ]\n",
    "        )\n",
    "        .agg(\n",
    "            geometry=(\"geometry\", \"first\"),\n",
    "            grid=(\"grid\", \"first\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "df_pivot = gpd.GeoDataFrame(df_pivot, geometry=df_pivot['geometry'])\n",
    "print(len(df_pivot))\n",
    "df_pivot.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5edb8a42",
   "metadata": {},
   "source": [
    "## Shock\n",
    "\n",
    "We will only work with the list of hazards that are enabled for our study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75faf930",
   "metadata": {},
   "outputs": [],
   "source": [
    "shock_dict = {\n",
    "    # \"floods\": f\"{PATH_DATA}/data/{COUNTRY_NAME}/original_sources/shocks_shapes/1_floods/Áreas_de_exposición.shp\",\n",
    "    \"1-5\": f\"{PATH_DATA}/data/{COUNTRY_NAME}/original_sources/shocks_shapes_floods_return/shapes_1_mas/Flood_1in5/Flood_1in5.shp\",\n",
    "    \"1-500\": f\"{PATH_DATA}/data/{COUNTRY_NAME}/original_sources/shocks_shapes_floods_return/shapes_1_mas/Flood_1in500/Flood_1in500.shp\",\n",
    "    \"1-1000\": f\"{PATH_DATA}/data/{COUNTRY_NAME}/original_sources/shocks_shapes_floods_return/shapes_1_mas/Flood_1in1000/Flood_1in1000.shp\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27384ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOCK_NAME = '1-5'\n",
    "\n",
    "path_shock_file = shock_dict[SHOCK_NAME]\n",
    "shock_shape = fsupport.shock_shape(path=path_shock_file, shock_name=SHOCK_NAME)\n",
    "shock_shape = shock_shape.unary_union\n",
    "\n",
    "shock_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43936a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shock_dict.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22e64924",
   "metadata": {},
   "source": [
    "### Temperatures per shock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8612a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = pd.DataFrame()\n",
    "\n",
    "for SHOCK_NAME in shock_dict.keys():\n",
    "# for SHOCK_NAME in list(shock_dict.keys())[:1]:\n",
    "    print(\"Shock: \" + SHOCK_NAME)\n",
    "\n",
    "    path_resp_shock = (\n",
    "        PATH_DATA + \"/data/\" + COUNTRY_NAME + \"/\" + RESULT_DIR + \"/\" + SHOCK_NAME\n",
    "    )\n",
    "    if not os.path.exists(path_resp_shock):\n",
    "        os.makedirs(path_resp_shock)\n",
    "\n",
    "    # Reading shape\n",
    "    path_shock_file = shock_dict[SHOCK_NAME]\n",
    "    shock_shape = fsupport.shock_shape(path=path_shock_file, shock_name=SHOCK_NAME)\n",
    "    shock_shape = shock_shape.unary_union\n",
    "    \n",
    "    # Intersection\n",
    "    points_within_polygon = df_pivot[df_pivot.geometry.within(shock_shape)]['hashed_geometry'].values\n",
    "    filtered_gdf = temp_shape[temp_shape.hashed_geometry.isin(points_within_polygon)]\n",
    "    data_temp = (\n",
    "        filtered_gdf.groupby(\n",
    "            [\n",
    "                \"day\",\n",
    "            ]\n",
    "        )\n",
    "        .agg(\n",
    "            temperature_mean=(\"temperature\", \"mean\"),\n",
    "            temperature_std=(\"temperature\", \"std\"),\n",
    "            temperature_max=(\"temperature\", \"max\"),\n",
    "            temperature_min=(\"temperature\", \"min\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    data_temp['shock'] = SHOCK_NAME\n",
    "    # time_series.to_csv( f\"temp_{SHOCK_NAME}.csv\", index=False)\n",
    "    \n",
    "    time_series = pd.concat([time_series, data_temp])\n",
    "    print()\n",
    "\n",
    "\n",
    "time_series.to_csv(f\"{directory_temperature}/temperature_day_table.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17f313e3",
   "metadata": {},
   "source": [
    "## Time Serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = pd.read_csv(f\"{directory_temperature}/temperature_day_table.csv\")\n",
    "time_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# import kaleido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db49df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Obtener la lista de colores únicos en el gráfico\n",
    "# unique_colors = time_series['shock'].unique()\n",
    "\n",
    "# # Asignar un tipo de línea diferente para cada color\n",
    "# for i, color in enumerate(unique_colors):\n",
    "#     fig.update_traces(selector=dict(name=color), line=dict(dash='dash' if i % 2 == 0 else 'dot'))\n",
    "\n",
    "\n",
    "fig = px.line(time_series, x=\"day\", y=\"temperature_mean\", color='shock')\n",
    "fig.update_layout(\n",
    "    width=1200,  \n",
    "    height=450  \n",
    ")\n",
    "fig.write_image(f\"{directory_temperature}/temperatura_year_mean_figure.png\",  engine='kaleido')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.line(time_series, x=\"day\", y=\"temperature_mean\", color='shock')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.bar(time_series, x='day', y='temperature_mean', color='shock', height=400)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fechas_generadas = generar_fechas(time_series['day'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series['date'] = time_series['day'].apply(lambda x: fechas_generadas[x-1])\n",
    "\n",
    "\n",
    "def group_by_month_year(dataframe):\n",
    "    dataframe['year'] = dataframe['date'].dt.year\n",
    "    dataframe['month'] = dataframe['date'].dt.month\n",
    "    \n",
    "    grouped_data = dataframe.groupby(['shock', 'year', 'month']).agg(\n",
    "                    temperature_mean=(\"temperature_mean\", \"mean\"),\n",
    "                    temperature_std=(\"temperature_std\", \"max\"),\n",
    "                    temperature_max=(\"temperature_max\", \"max\"),\n",
    "                    temperature_min=(\"temperature_min\", \"min\"),\n",
    "                    ).reset_index()\n",
    "    return grouped_data\n",
    "\n",
    "df_grouped = group_by_month_year(time_series)\n",
    "df_grouped['date'] = pd.to_datetime(df_grouped[['year', 'month']].assign(day=1))\n",
    "df_grouped.to_csv(f\"{directory_temperature}/temperatura_montly_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da1897",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_grouped, x=[\"date\"], y=\"temperature_mean\", color='shock')\n",
    "# Obtener la lista de colores únicos en el gráfico\n",
    "unique_colors = df_grouped['shock'].unique()\n",
    "\n",
    "# Asignar un tipo de línea diferente para cada color\n",
    "for i, color in enumerate(unique_colors):\n",
    "    fig.update_traces(selector=dict(name=color), line=dict(dash='dash' if i % 2 == 0 else 'dot'))\n",
    "fig.update_layout(\n",
    "    width=1200,  \n",
    "    height=450  \n",
    ")\n",
    "fig.write_image(f\"{directory_temperature}/temperatura_montly_mean_figure.png\",  engine='kaleido')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e02bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_grouped, x=[\"date\"], y=\"temperature_min\", color='shock')\n",
    "# Obtener la lista de colores únicos en el gráfico\n",
    "unique_colors = df_grouped['shock'].unique()\n",
    "\n",
    "# Asignar un tipo de línea diferente para cada color\n",
    "for i, color in enumerate(unique_colors):\n",
    "    fig.update_traces(selector=dict(name=color), line=dict(dash='dash' if i % 2 == 0 else 'dot'))\n",
    "    \n",
    "fig.update_layout(\n",
    "    width=1200,  \n",
    "    height=450  \n",
    ")\n",
    "fig.write_image(f\"{directory_temperature}/temperatura_montly_min_figure.png\",  engine='kaleido')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9153b319",
   "metadata": {},
   "source": [
    "## Hospitals and temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "# from healthcare_facilities_priority import *\n",
    "import healthcare_facilities_priority\n",
    "importlib.reload(healthcare_facilities_priority)\n",
    "\n",
    "from shapely import wkt\n",
    "\n",
    "df_pivot_temp = df_pivot.copy()\n",
    "df_pivot_temp['centroid'] = df_pivot_temp['geometry']\n",
    "df_pivot_temp['geometry'] = df_pivot_temp['grid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e511ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shock_dict = {\n",
    "    # \"floods\": f\"{PATH_DATA}/data/{COUNTRY_NAME}/original_sources/shocks_shapes/1_floods/Áreas_de_exposición.shp\",\n",
    "    \"1-5\": f\"{PATH_DATA}/data/{COUNTRY_NAME}/original_sources/shocks_shapes_floods_return/shapes_1_mas/Flood_1in5/Flood_1in5.shp\",\n",
    "    \"1-500\": f\"{PATH_DATA}/data/{COUNTRY_NAME}/original_sources/shocks_shapes_floods_return/shapes_1_mas/Flood_1in500/Flood_1in500.shp\",\n",
    "    \"1-1000\": f\"{PATH_DATA}/data/{COUNTRY_NAME}/original_sources/shocks_shapes_floods_return/shapes_1_mas/Flood_1in1000/Flood_1in1000.shp\",\n",
    "}\n",
    "shock_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92e0879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Read exposed population\n",
    "# df_exposed_population = pd.read_csv('peru_exposed_population_floods_adm3_table.csv')\n",
    "# #Read exposed healthcare infrastructure\n",
    "# df_health_infrastructure = pd.read_csv('peru_health_infrastructure_floods_table.csv')\n",
    "# #Read danger (flood, landslides, etc) variables\n",
    "# df_danger_variables =  pd.read_csv('peru_floods_variables_table.csv')\n",
    "\n",
    "factor_variables = ['Administration 3 Code','Per Indigenous Pop', 'Per Pobrezanbi', \n",
    "             'Per Desnutricion', 'ValorConcentracióndeEESSdel', 'ValorEESSconmayorcapacidadr', \n",
    "             'MEDICOX10000HABITANTENORMAL','Per Ess 100hab', 'PHC Dist Km', 'Hospital Dist Km']\n",
    "\n",
    "variables_for_prioritization = ['Exposed Pop','Per Indigenous Pop','Per Pobrezanbi', 'Per Desnutricion', 'ValorConcentracióndeEESSdel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = pd.DataFrame()\n",
    "\n",
    "for SHOCK_NAME in shock_dict.keys():\n",
    "# for SHOCK_NAME in list(shock_dict.keys())[:1]:\n",
    "    print(\"Shock: \" + SHOCK_NAME)\n",
    "    \n",
    "    # Read exposed population\n",
    "    path_resp_shock = f\"{directory}/{SHOCK_NAME}/peru_exposed_population_{SHOCK_NAME}_adm3_table.csv\"\n",
    "    df_exposed_population = pd.read_csv(path_resp_shock, dtype={\n",
    "                                                                \"Administration 3 Code\": str,\n",
    "                                                            },)\n",
    "    \n",
    "    # Read danger (flood, landslides, etc) variables\n",
    "    path_resp_shock = f\"{directory}/{SHOCK_NAME}/peru_health_infrastructure_{SHOCK_NAME}_table.csv\"\n",
    "    df_health_infrastructure = pd.read_csv(path_resp_shock, dtype={\n",
    "                                                                \"Administration 3 Code\": str,\n",
    "                                                            },)\n",
    "    df_health_infrastructure =  pd.merge(\n",
    "                                    df_health_infrastructure,\n",
    "                                    administrative_areas_dict['Administration 3'][['administration_3_code','administration_1','administration_2','administration_3']],\n",
    "                                    how=\"left\", \n",
    "                                    left_on=['Administration 3 Code'], \n",
    "                                    right_on=['administration_3_code']\n",
    "                                )\n",
    "\n",
    "    df_health_infrastructure = gpd.GeoDataFrame(df_health_infrastructure)\n",
    "    df_health_infrastructure['geometry'] = df_health_infrastructure['geometry'].apply(wkt.loads)\n",
    "    \n",
    "    # Read danger (flood, landslides, etc) variables\n",
    "    path_resp_shock = f\"{directory}/{SHOCK_NAME}/peru_{SHOCK_NAME}_variables_table.csv\"\n",
    "    df_danger_variables = pd.read_csv(path_resp_shock)\n",
    "    \n",
    "    df_health_infrastructure = healthcare_facilities_priority.healthcare_prioritization(df_exposed_population,df_health_infrastructure,df_danger_variables,factor_variables)\n",
    "\n",
    "    top = 10\n",
    "    healthcare_facilities_priority.prioritization_national_level(df_health_infrastructure,\n",
    "                                  variables_for_prioritization,\n",
    "                                  top,\n",
    "                                  \"Peru\", \n",
    "                                  SHOCK_NAME, \n",
    "                                  f\"{directory_prioritization}\")\n",
    "    \n",
    "    pxd_phc, pxd_hosp = healthcare_facilities_priority.prioritization_by_department_level(df_health_infrastructure,\n",
    "                                                                      variables_for_prioritization,\n",
    "                                                                      \"Peru\",\n",
    "                                                                      SHOCK_NAME,\n",
    "                                                                      f\"{directory_prioritization}\")\n",
    "    \n",
    "    # Temperatures\n",
    "    \n",
    "    if SHOCK_NAME in [\"frost\", \"extreme cold\", \"snow\"]:\n",
    "        hospital_temp = df_health_infrastructure[['Hospital', 'geometry']].copy()\n",
    "        hospital_temp = gpd.sjoin(hospital_temp,\n",
    "                                df_pivot_temp[['hashed_geometry','geometry']], \n",
    "                                how='inner', \n",
    "                                predicate='intersects')\n",
    "\n",
    "        hospital_temp = pd.merge(\n",
    "                            hospital_temp[['Hospital', 'hashed_geometry']],\n",
    "                            temp_shape[['day','temperature', 'hashed_geometry']],\n",
    "                            how=\"left\", \n",
    "                            left_on=['hashed_geometry'], \n",
    "                            right_on=['hashed_geometry']\n",
    "                        )\n",
    "\n",
    "        hospital_temp = group_hospitals_by_hosp_month_year(hospital_temp)\n",
    "\n",
    "\n",
    "\n",
    "        hospital_temp = hospital_temp.pivot(index=['Hospital'], \n",
    "                                            columns=['year']\n",
    "                                        ).reset_index()\n",
    "        hospital_temp.columns = hospital_temp.columns.to_flat_index()\n",
    "        hospital_temp.columns = hospital_temp.columns.map(lambda x: f\"{x[0]}_{x[1]}\")\n",
    "\n",
    "        hospital_temp = hospital_temp.rename(\n",
    "            columns={\"Hospital_\": \"Hospital\",}\n",
    "        )\n",
    "\n",
    "        hospital_temp = pd.merge(\n",
    "                            df_health_infrastructure,\n",
    "                            hospital_temp,\n",
    "                            how=\"left\", \n",
    "                            left_on=['Hospital'], \n",
    "                            right_on=['Hospital']\n",
    "                        )\n",
    "\n",
    "        path = f\"{directory_temperature}/Peru_{SHOCK_NAME}_hospitals_temperature_table.csv\"\n",
    "        hospital_temp.to_csv(path, index=False)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be33352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_health_infrastructure\n",
    "# variables_for_prioritization\n",
    "# df_health_infrastructure[df_health_infrastructure['Shock Name'] == 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5722c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_health_infrastructure[(df_health_infrastructure['Shock Name']==1) &\n",
    "                             (df_health_infrastructure['Nivel']!=1)]['administration_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_health_infrastructure[(df_health_infrastructure['Shock Name']==1) &\n",
    "                             (df_health_infrastructure['Nivel']!=1) &\n",
    "                             (df_health_infrastructure['administration_1']=='APURIMAC')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f015aec0",
   "metadata": {},
   "source": [
    "### matriz de exposición de hospital a peligros compuestos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hazards = list(shock_dict.keys())\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa556beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matix_phc = []\n",
    "for SHOCK_NAME_RAW in list_hazards:\n",
    "    \n",
    "    raw_phc_list = []\n",
    "    raw_phc_list.append(SHOCK_NAME_RAW)\n",
    "    path = f\"{directory_prioritization}/Peru_PHC_priority_{SHOCK_NAME_RAW}.csv\"\n",
    "    phc_raw = pd.read_csv(path)\n",
    "    phc_raw = set(phc_raw['Hospital'])\n",
    "    \n",
    "    \n",
    "    for SHOCK_NAME_COL in list_hazards:\n",
    "        \n",
    "        path = f\"{directory_prioritization}/Peru_PHC_priority_{SHOCK_NAME_COL}.csv\"\n",
    "        phc_col = pd.read_csv(path)\n",
    "        phc_col = set(phc_col['Hospital'])\n",
    "        \n",
    "        intersections = len(phc_raw.intersection(phc_col))\n",
    "        raw_phc_list.append(intersections)\n",
    "    \n",
    "    matix_phc.append(raw_phc_list)\n",
    "    \n",
    "csv_file_path = f\"{directory_prioritization}/Peru_matrix_PHC.csv\"\n",
    "with open(csv_file_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([''] + list_hazards)\n",
    "    for row in matix_phc:\n",
    "        writer.writerow(row)\n",
    "        \n",
    "matix_phc    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c708bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "matix_priority = []\n",
    "for SHOCK_NAME_RAW in list_hazards:\n",
    "    \n",
    "    raw_priority_list = []\n",
    "    raw_priority_list.append(SHOCK_NAME_RAW)\n",
    "    path = f\"{directory_prioritization}/Peru_Hospitals_priority_{SHOCK_NAME_RAW}.csv\"\n",
    "\n",
    "    priority_raw = pd.read_csv(path)\n",
    "    priority_raw = set(priority_raw['Hospital'])\n",
    "    \n",
    "    for SHOCK_NAME_COL in list_hazards:\n",
    "        \n",
    "        path = f\"{directory_prioritization}/Peru_Hospitals_priority_{SHOCK_NAME_COL}.csv\"\n",
    "        priority_col = pd.read_csv(path)\n",
    "        priority_col = set(priority_col['Hospital'])\n",
    "        intersections = len(priority_raw.intersection(priority_col))\n",
    "        raw_priority_list.append(intersections)\n",
    "    matix_priority.append(raw_priority_list)\n",
    "    \n",
    "csv_file_path = f\"{directory_prioritization}/Peru_matrix_priority.csv\"\n",
    "with open(csv_file_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([''] + list_hazards)\n",
    "    for row in matix_priority:\n",
    "        writer.writerow(row)\n",
    "          \n",
    "matix_priority "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71464814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matix_phc = []\n",
    "for SHOCK_NAME_RAW in list_hazards:\n",
    "    \n",
    "    raw_phc_list = []\n",
    "    raw_phc_list.append(SHOCK_NAME_RAW)\n",
    "    path = f\"{directory}/{SHOCK_NAME_RAW}/peru_health_infrastructure_{SHOCK_NAME_RAW}_table.csv\"\n",
    "    phc_raw = pd.read_csv(path)\n",
    "    phc_raw = phc_raw[(phc_raw['Nivel']==1) & (phc_raw['Shock Name']==1)]\n",
    "    phc_raw = set(phc_raw['Hospital'])\n",
    "    \n",
    "    \n",
    "    for SHOCK_NAME_COL in list_hazards:\n",
    "        path = f\"{directory}/{SHOCK_NAME_COL}/peru_health_infrastructure_{SHOCK_NAME_COL}_table.csv\"\n",
    "        phc_col = pd.read_csv(path)\n",
    "        phc_col = phc_col[(phc_col['Nivel']==1) & (phc_col['Shock Name']==1)]\n",
    "        phc_col = set(phc_col['Hospital'])\n",
    "        \n",
    "        intersections = len(phc_raw.intersection(phc_col))\n",
    "        raw_phc_list.append(intersections)\n",
    "    \n",
    "    matix_phc.append(raw_phc_list)\n",
    "    \n",
    "csv_file_path = f\"{directory_prioritization}/Peru_matrix_PHC.csv\"\n",
    "with open(csv_file_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([''] + list_hazards)\n",
    "    for row in matix_phc:\n",
    "        writer.writerow(row)\n",
    "        \n",
    "matix_phc    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd49bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "matix_priority = []\n",
    "for SHOCK_NAME_RAW in list_hazards:\n",
    "    \n",
    "    raw_priority_list = []\n",
    "    raw_priority_list.append(SHOCK_NAME_RAW)\n",
    "    path = f\"{directory}/{SHOCK_NAME_RAW}/peru_health_infrastructure_{SHOCK_NAME_RAW}_table.csv\"\n",
    "    priority_raw = pd.read_csv(path)\n",
    "    priority_raw = priority_raw[(priority_raw['Nivel']!=1) & (priority_raw['Shock Name']==1)]\n",
    "    priority_raw = set(priority_raw['Hospital'])\n",
    "    \n",
    "    for SHOCK_NAME_COL in list_hazards:\n",
    "        path = f\"{directory}/{SHOCK_NAME_COL}/peru_health_infrastructure_{SHOCK_NAME_COL}_table.csv\"\n",
    "        priority_col = pd.read_csv(path)\n",
    "        priority_col = priority_col[(priority_col['Nivel']!=1) & (priority_col['Shock Name']==1)]\n",
    "        priority_col = set(priority_col['Hospital'])\n",
    "        intersections = len(priority_raw.intersection(priority_col))\n",
    "        raw_priority_list.append(intersections)\n",
    "    matix_priority.append(raw_priority_list)\n",
    "    \n",
    "csv_file_path = f\"{directory_prioritization}/Peru_matrix_priority.csv\"\n",
    "with open(csv_file_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([''] + list_hazards)\n",
    "    for row in matix_priority:\n",
    "        writer.writerow(row)\n",
    "          \n",
    "matix_priority "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7c8c6d6",
   "metadata": {},
   "source": [
    "### Voronoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ece4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{PATH_DATA}/data/peru/preprocessed_sources/administrative_region\"\n",
    "files = os.listdir(path)\n",
    "\n",
    "administrative_areas_dict = dict()\n",
    "for i in files:\n",
    "    name = fsupport.capitalize_string(i[:-8])\n",
    "    administrative_areas_dict[name] = gpd.read_file(path + \"/\" + i)\n",
    "\n",
    "administrative_areas = administrative_areas_dict[max(administrative_areas_dict.keys())]\n",
    "\n",
    "filtered_columns = [\n",
    "    col\n",
    "    for col in administrative_areas.columns\n",
    "    if col.endswith(\"code\") or col == \"geometry\"\n",
    "]\n",
    "administrative_areas = administrative_areas[filtered_columns]\n",
    "administrative_areas = administrative_areas.reindex(\n",
    "    sorted(administrative_areas.columns), axis=1\n",
    ")\n",
    "administrative_areas = administrative_areas.to_crs(\"EPSG:4326\")\n",
    "administrative_areas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff007be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_missing_elements(lst):\n",
    "    # Obtén la lista de todos los números presentes en la lista original\n",
    "    numbers = [sublst[0] for sublst in lst]\n",
    "\n",
    "    # Encuentra el valor mínimo y máximo de esos números\n",
    "    min_number = min(numbers)\n",
    "    max_number = max(numbers)\n",
    "\n",
    "    # Crea una nueva lista con todos los números en el rango desde el mínimo hasta el máximo\n",
    "    complete_lst = [[num, None] for num in range(min_number, max_number + 1)]\n",
    "\n",
    "    # Itera sobre la lista original y agrega elementos None en la posición correspondiente\n",
    "    for sublst in lst:\n",
    "        number = sublst[0]\n",
    "        value = sublst[1]\n",
    "        complete_lst[number - min_number][1] = value\n",
    "\n",
    "    return complete_lst\n",
    "\n",
    "\n",
    "def hash_point(point):\n",
    "    decimales = 5\n",
    "    lat, lon = round(point.x, decimales), round(point.y, decimales)\n",
    "    # lat, lon = point.x, point.y\n",
    "    coordinate_str = f\"{lat},{lon}\"\n",
    "    # hash_object = hashlib.sha256(coordinate_str.encode())\n",
    "    # return hash_object.hexdigest()\n",
    "    return coordinate_str\n",
    "\n",
    "\n",
    "def area_shape_function(administrative_areas):\n",
    "    area = administrative_areas.copy()\n",
    "    area = area.to_crs(epsg=4326)\n",
    "    area = area.unary_union\n",
    "    area = gpd.GeoDataFrame(geometry=[area])\n",
    "    area_shape = area.iloc[0].geometry\n",
    "    return area_shape\n",
    "\n",
    "\n",
    "def gdf_voronoi_function(data, area_shape):\n",
    "    gdf = data.copy()\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "    print(\"Total de elementos:\", len(gdf))\n",
    "    gdf['hash'] = gdf['geometry'].apply(hash_point)\n",
    "    gdf.drop_duplicates(subset='hash', keep='first', inplace=True)\n",
    "    del gdf['hash']\n",
    "    print(\"Eliminando puntos duplicados:\", len(gdf))\n",
    "    gdf = gdf[gdf.within(area_shape)].reset_index(drop=True)\n",
    "    print(\"Eliminando areas fuera del poligono:\", len(gdf))\n",
    "    \n",
    "    coords = points_to_coords(gdf.geometry)\n",
    "    unique_coords, unique_indices = np.unique(coords, axis=0, return_index=True)\n",
    "    region_polys, region_pts = voronoi_regions_from_coords(coords, area_shape)\n",
    "    \n",
    "    elements = []\n",
    "    for i in range(0,len(region_polys)):\n",
    "        pol = region_polys[i]\n",
    "        lista = region_pts[i]\n",
    "        for ind in lista:\n",
    "            elements.append([ind, pol])\n",
    "\n",
    "    elements = sorted(elements, key=lambda x: x[0])\n",
    "    elements = complete_missing_elements(elements)\n",
    "    elements = [e[1] for e in elements]\n",
    "\n",
    "    gdf['geometry'] = elements\n",
    "    gdf = gdf[~(gdf[\"geometry\"].isna())].reset_index(drop=True)\n",
    "    \n",
    "    print(\"Poligonos procesados:\", len(gdf))\n",
    "    return gdf[['hospital','geometry']]\n",
    "\n",
    "\n",
    "# def gdf_voronoi_function2(data, area_shape):\n",
    "#     gdf = data.copy()\n",
    "#     gdf = gdf.to_crs(epsg=4326)\n",
    "    \n",
    "#     coordinates = np.array(gdf['geometry'].apply(lambda geom: (geom.centroid.x, geom.centroid.y)).to_list())\n",
    "#     vor = Voronoi(coordinates)\n",
    "#     # coordinates\n",
    "\n",
    "#     gdf['region'] = None\n",
    "#     for i, region_index in enumerate(vor.point_region):\n",
    "#         vertex_indices = vor.regions[region_index]\n",
    "#         if -1 not in vertex_indices and len(vertex_indices) >= 3:\n",
    "#             vertices = vor.vertices[vertex_indices]\n",
    "#             polygon = gpd.GeoSeries([Polygon(vertices)]).iloc[0]\n",
    "#             gdf.at[i, 'region'] = polygon\n",
    "#     gdf = gdf[~(gdf['region'].isna())].reset_index(drop=True)\n",
    "#     gdf['geometry'] = gdf['region'] \n",
    "#     gdf = gdf[['hospital','geometry']]\n",
    "#     gdf = gdf[gdf['geometry'].intersects(area_shape)]\n",
    "#     gdf = gdf.reset_index(drop=True)\n",
    "#     return gdf\n",
    "\n",
    "\n",
    "def voronoi_join_nearest_points(\n",
    "    population_shock_df, country_hospitals_voronoi,\n",
    "):\n",
    "    country_hospitals_voronoi[\"region\"] = country_hospitals_voronoi.geometry\n",
    "    data_temp = gpd.sjoin(population_shock_df, country_hospitals_voronoi, how='left', predicate=\"within\")\n",
    "    \n",
    "    # Parte 1\n",
    "    data_temp1 = data_temp[~(data_temp[\"region\"].isna())].reset_index(drop=True)\n",
    "    del data_temp1[\"index_right\"]\n",
    "    \n",
    "    # Parte 2\n",
    "    data_temp2 = data_temp[(data_temp[\"region\"].isna())].reset_index(drop=True) \n",
    "    data_temp2 = data_temp2[population_shock_df.columns]\n",
    "    data_temp2 = gpd.sjoin_nearest(data_temp2, country_hospitals_voronoi, how='left', distance_col=\"distances\")\n",
    "    del data_temp2[\"index_right\"]\n",
    "    del data_temp2[\"distances\"]\n",
    "\n",
    "    data_temp = pd.concat([data_temp1, data_temp2])\n",
    "    data_temp['hospital'] = data_temp['hospital'].astype(int)\n",
    "    # data_temp['hospital'] = data_temp['hospital'].astype(str)\n",
    "\n",
    "    return data_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c279818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import cascaded_union, unary_union\n",
    "\n",
    "from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area\n",
    "from geovoronoi import voronoi_regions_from_coords, points_to_coords\n",
    "\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "from shapely.wkt import loads\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "import hashlib\n",
    "\n",
    "catchment_area = f\"{directory}/catchment_area\"\n",
    "if not os.path.exists(catchment_area):\n",
    "    os.makedirs(catchment_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1aa3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bbd9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Areas\n",
    "area_shape = area_shape_function(administrative_areas)\n",
    "\n",
    "# Hospitals\n",
    "path = f\"{PATH_DATA}/data/peru/preprocessed_sources/hospitals.csv\"\n",
    "df_hospitals = pd.read_csv(path)\n",
    "df_hospitals = fsupport.hosp_join_points(df_hospitals, administrative_areas)\n",
    "df_hospitals = df_hospitals.dropna(subset=['geometry'])\n",
    "\n",
    "# Others \n",
    "print(\"PROCESSING HOSPITALS\")\n",
    "country_hospitals = df_hospitals[df_hospitals['nivel']!=1].reset_index(drop=True)\n",
    "country_hospitals_voronoi = gdf_voronoi_function(country_hospitals, area_shape)\n",
    "\n",
    "# Population for each region\n",
    "for SHOCK_NAME_RAW in list_hazards + ['total']:\n",
    "    print(\"  \",SHOCK_NAME_RAW)\n",
    "    if 'total' == SHOCK_NAME_RAW:\n",
    "        # population_shock_df = gpd.GeoDataFrame(population_shock_df, crs=\"EPSG:4326\")\n",
    "        path = PATH_DATA + \"/data/peru/preprocessed_sources/population.geojson\"\n",
    "        population_shock_df = gpd.read_file(path)\n",
    "        population_shock_df = fsupport.extract_centroid_if_exist(population_shock_df)\n",
    "        population_shock_df = population_shock_df[['population','geometry']]\n",
    "        population_shock_df = population_shock_df.rename(columns={\"population\": 'Population'})\n",
    "        population_shock_df['population_id'] = population_shock_df.index\n",
    "        data_temp = voronoi_join_nearest_points(population_shock_df, country_hospitals_voronoi)\n",
    "        data_temp = data_temp.groupby(['hospital']).agg(\n",
    "                                                Population=(\"Population\", \"sum\")\n",
    "                                            ).reset_index()\n",
    "        data_temp = data_temp.rename(columns={\"Population\": SHOCK_NAME_RAW})\n",
    "        country_hospitals_voronoi_ = pd.merge(\n",
    "                                country_hospitals_voronoi,\n",
    "                                data_temp,\n",
    "                                how=\"left\", \n",
    "                                left_on=['hospital'], \n",
    "                                right_on=['hospital']\n",
    "                            )\n",
    "        country_hospitals_voronoi[SHOCK_NAME_RAW] = country_hospitals_voronoi_[SHOCK_NAME_RAW].fillna(0)\n",
    "        country_hospitals_voronoi[SHOCK_NAME_RAW] = country_hospitals_voronoi[SHOCK_NAME_RAW].astype(int)\n",
    "    else:\n",
    "        path = f\"{directory}/{SHOCK_NAME_RAW}/peru_population_points_{SHOCK_NAME_RAW}_table.csv\"\n",
    "        population_shock_df = pd.read_csv(path)\n",
    "        population_shock_df = population_shock_df[population_shock_df['Shock Name']==1]\n",
    "        population_shock_df = population_shock_df.iloc[:,1:]\n",
    "        population_shock_df['geometry'] = population_shock_df['geometry'].apply(lambda x: loads(x))\n",
    "        population_shock_df = gpd.GeoDataFrame(population_shock_df, crs=\"EPSG:4326\")\n",
    "        population_shock_df['population_id'] = population_shock_df.index\n",
    "        data_temp = voronoi_join_nearest_points(population_shock_df, country_hospitals_voronoi)\n",
    "        data_temp = data_temp.groupby(['hospital']).agg(\n",
    "                                                Population=(\"Population\", \"sum\")\n",
    "                                            ).reset_index()\n",
    "        data_temp = data_temp.rename(columns={\"Population\": SHOCK_NAME_RAW})\n",
    "        country_hospitals_voronoi_ = pd.merge(\n",
    "                                country_hospitals_voronoi,\n",
    "                                data_temp,\n",
    "                                how=\"left\", \n",
    "                                left_on=['hospital'], \n",
    "                                right_on=['hospital']\n",
    "                            )\n",
    "        country_hospitals_voronoi[SHOCK_NAME_RAW] = country_hospitals_voronoi_[SHOCK_NAME_RAW].fillna(0)\n",
    "        country_hospitals_voronoi[SHOCK_NAME_RAW] = country_hospitals_voronoi[SHOCK_NAME_RAW].astype(float)\n",
    "    \n",
    "# Save file\n",
    "path = f\"{catchment_area}/peru_population_hospital_voronoi_table.csv\"\n",
    "country_hospitals_voronoi.to_csv(path, index = False)\n",
    "\n",
    "################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
    "\n",
    "# PHC\n",
    "print(\"PROCESSING PHC\")\n",
    "country_phc = df_hospitals[df_hospitals['nivel']==1].reset_index(drop=True)\n",
    "country_phc_voronoi = gdf_voronoi_function(country_phc, area_shape)\n",
    "\n",
    "# Population for each region\n",
    "for SHOCK_NAME_RAW in list_hazards + ['total']:\n",
    "    print(\"  \",SHOCK_NAME_RAW)\n",
    "    if 'total' == SHOCK_NAME_RAW:\n",
    "        path = PATH_DATA + \"/data/peru/preprocessed_sources/population.geojson\"\n",
    "        population_shock_df = gpd.read_file(path)\n",
    "        population_shock_df = fsupport.extract_centroid_if_exist(population_shock_df)\n",
    "        population_shock_df = population_shock_df[['population','geometry']]\n",
    "        population_shock_df = population_shock_df.rename(columns={\"population\": 'Population'})\n",
    "        population_shock_df['population_id'] = population_shock_df.index\n",
    "        data_temp = voronoi_join_nearest_points(population_shock_df, country_phc_voronoi)\n",
    "        data_temp = data_temp.groupby(['hospital']).agg(\n",
    "                                                Population=(\"Population\", \"sum\")\n",
    "                                            ).reset_index()\n",
    "        data_temp = data_temp.rename(columns={\"Population\": SHOCK_NAME_RAW})\n",
    "        country_phc_voronoi_ = pd.merge(\n",
    "                                country_phc_voronoi,\n",
    "                                data_temp,\n",
    "                                how=\"left\", \n",
    "                                left_on=['hospital'], \n",
    "                                right_on=['hospital']\n",
    "                            )\n",
    "        country_phc_voronoi[SHOCK_NAME_RAW] = country_phc_voronoi_[SHOCK_NAME_RAW].fillna(0)\n",
    "        country_phc_voronoi[SHOCK_NAME_RAW] = country_phc_voronoi[SHOCK_NAME_RAW].astype(int)\n",
    "    else:\n",
    "        path = f\"{directory}/{SHOCK_NAME_RAW}/peru_population_points_{SHOCK_NAME_RAW}_table.csv\"\n",
    "        population_shock_df = pd.read_csv(path)\n",
    "        population_shock_df = population_shock_df[population_shock_df['Shock Name']==1]\n",
    "        population_shock_df = population_shock_df.iloc[:,1:]\n",
    "        population_shock_df['geometry'] = population_shock_df['geometry'].apply(lambda x: loads(x))\n",
    "        population_shock_df = gpd.GeoDataFrame(population_shock_df, crs=\"EPSG:4326\")\n",
    "        population_shock_df['population_id'] = population_shock_df.index\n",
    "        # data_temp = gpd.sjoin(population_shock_df, country_phc_voronoi, predicate='within')\n",
    "        data_temp = voronoi_join_nearest_points(population_shock_df, country_phc_voronoi)\n",
    "        data_temp = data_temp.groupby(['hospital']).agg(\n",
    "                                                Population=(\"Population\", \"sum\")\n",
    "                                            ).reset_index()\n",
    "        data_temp = data_temp.rename(columns={\"Population\": SHOCK_NAME_RAW})\n",
    "        country_phc_voronoi_ = pd.merge(\n",
    "                                country_phc_voronoi,\n",
    "                                data_temp,\n",
    "                                how=\"left\", \n",
    "                                left_on=['hospital'], \n",
    "                                right_on=['hospital']\n",
    "                            )\n",
    "        country_phc_voronoi[SHOCK_NAME_RAW] = country_phc_voronoi_[SHOCK_NAME_RAW].fillna(0)\n",
    "        country_phc_voronoi[SHOCK_NAME_RAW] = country_phc_voronoi[SHOCK_NAME_RAW].astype(float)\n",
    "\n",
    "# Save file\n",
    "path = f\"{catchment_area}/peru_population_phc_voronoi_table.csv\"\n",
    "country_phc_voronoi.to_csv(path, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce30c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_phc_voronoi['total'].sum(), country_hospitals_voronoi['total'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_scale(df, col_name):\n",
    "    norm = mcolors.Normalize(\n",
    "        vmin=min(df[col_name].dropna()), vmax=max(df[col_name].dropna())\n",
    "    )\n",
    "    return norm\n",
    "    \n",
    "for col_ in list_hazards+ ['total']:\n",
    "    # data_temp = country_phc_voronoi.copy()\n",
    "    data_temp = country_hospitals_voronoi.copy()\n",
    "    col = col_\n",
    "    col_g = col + \"_g\"\n",
    "    data_temp[col_g] = data_temp[col].apply(lambda x: x ** (1/10))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    ax.axis(\"off\")\n",
    "    cmap_color = \"viridis\"\n",
    "    norm = get_color_scale(data_temp, col_g)\n",
    "    plot = data_temp.plot(\n",
    "        ax=ax,\n",
    "        column=col_g,\n",
    "        cmap=cmap_color,\n",
    "    )\n",
    "    ctx.add_basemap(\n",
    "        plot, crs=\"epsg:4326\", source=ctx.providers.Stamen.TonerBackground, attribution=\"\"\n",
    "    )\n",
    "    A = [-70.1 * np.pi / 180.0, -2.5 * np.pi / 180.0]\n",
    "    B = [-70.8 * np.pi / 180.0, -2.5 * np.pi / 180.0]\n",
    "    dx = (6371000) * haversine_distances([A, B])[0, 1]\n",
    "    ax.add_artist(ScaleBar(dx=dx, units=\"m\"))\n",
    "    ticks = [\n",
    "        max(data_temp[col].dropna()),\n",
    "        ( max(data_temp[col].dropna()) - ((max(data_temp[col].dropna()) - min(data_temp[col].dropna())) / 2)),\n",
    "        min(data_temp[col].dropna()),\n",
    "    ]\n",
    "    ticks = np.linspace(min(data_temp[col].dropna()),  max(data_temp[col].dropna()), 10)\n",
    "    ticks_labels = [f\"{x/1000:.2f}K\" for x in ticks]  # Convert the values to thousands and format them as strings\n",
    "    cax = fig.add_axes(\n",
    "        [ax.get_position().x1 + 0.05, ax.get_position().y0, 0.01, ax.get_position().height]\n",
    "    )\n",
    "\n",
    "    cbar = fig.colorbar(\n",
    "        cm.ScalarMappable(norm=norm, cmap=cmap_color),\n",
    "        cax=cax,\n",
    "        orientation=\"vertical\",\n",
    "        label=\"\",\n",
    "        boundaries=np.linspace(\n",
    "            min(data_temp[col_g].dropna()), max(data_temp[col_g].dropna()), 10\n",
    "        ),\n",
    "    )\n",
    "    cbar.ax.set_yticklabels(ticks_labels)\n",
    "    cbar.set_label(\"Number of people (in thousands)\")\n",
    "    \n",
    "    # path = f\"{catchment_area}/peru_population_phc_voronoi_{col}_figure.png\"\n",
    "    path = f\"{catchment_area}/peru_population_hospital_voronoi_{col}_figure.png\"\n",
    "    #country_hospitals_voronoi.to_csv(path, index = False)\n",
    "    plt.savefig(path, format='png', dpi = 150, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5a99d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
